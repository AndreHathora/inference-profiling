import time
import logging
import sys
import os
import traceback
from typing import List, Dict, Any, Optional
from src.core.benchmark_core import BaseBenchmark

# Set up debug logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Extensive debugging for SGLang import
logger.info("=== SGLang Import Debug Start ===")
logger.info(f"Python path: {sys.path}")
logger.info(f"Current working directory: {os.getcwd()}")

try:
    logger.info("Attempting to import sglang...")
    from sglang.srt.entrypoints.engine import Engine
    SGLANG_AVAILABLE = True
    logger.info("âœ… SGLang successfully imported")
    logger.info(f"Engine class: {Engine}")
    logger.info(f"Engine module: {Engine.__module__}")
    logger.info(f"Engine attributes: {dir(Engine)}")
except ImportError as ie:
    logger.error(f"âŒ SGLang ImportError: {ie}")
    logger.error(f"ImportError type: {type(ie)}")
    SGLANG_AVAILABLE = False
    Engine = None
except Exception as e:
    logger.error(f"âŒ SGLang import failed with unexpected error: {e}")
    logger.error(f"Exception type: {type(e)}")
    logger.error(f"Exception args: {e.args}")
    logger.error(f"Exception traceback: {traceback.format_exc()}")
    SGLANG_AVAILABLE = False
    Engine = None

logger.info(f"Final SGLANG_AVAILABLE: {SGLANG_AVAILABLE}")
logger.info(f"Final Engine: {Engine}")
logger.info("=== SGLang Import Debug End ===")

# Final debug check after everything is loaded
logger.info("=== Final SGLang State Check ===")
logger.info(f"SGLANG_AVAILABLE in globals(): {'SGLANG_AVAILABLE' in globals()}")
if 'SGLANG_AVAILABLE' in globals():
    logger.info(f"SGLANG_AVAILABLE value: {globals()['SGLANG_AVAILABLE']}")
else:
    logger.warning("SGLANG_AVAILABLE not found in globals!")

logger.info(f"Engine in globals(): {'Engine' in globals()}")
if 'Engine' in globals():
    logger.info(f"Engine value: {globals()['Engine']}")
else:
    logger.warning("Engine not found in globals!")

logger.info("=== Final SGLang State Check End ===")


class SGLangBenchmark(BaseBenchmark):
    def __init__(self, model_name: str, tp_size: int = 1, max_model_len: int = None,
                 max_total_tokens: int = 4096, max_tokens: int = 100):
        logger.info("=== SGLangBenchmark __init__ Debug Start ===")
        logger.info(f"Initializing with model: {model_name}, tp_size: {tp_size}")

        super().__init__(model_name, max_tokens)
        self.tp_size = tp_size
        self.max_model_len = max_model_len
        self.max_total_tokens = max_total_tokens
        self.engine: Optional[Engine] = None

        logger.info(f"Super init completed. Engine attribute: {self.engine}")

        # Debug global variable access
        logger.info("Checking SGLANG_AVAILABLE global variable...")
        try:
            sglang_available = globals().get('SGLANG_AVAILABLE', 'NOT_FOUND')
            logger.info(f"globals().get('SGLANG_AVAILABLE'): {sglang_available}")
        except Exception as e:
            logger.error(f"Error accessing globals(): {e}")
            sglang_available = False

        # Try direct access too
        try:
            direct_available = globals()['SGLANG_AVAILABLE'] if 'SGLANG_AVAILABLE' in globals() else 'NOT_IN_GLOBALS'
            logger.info(f"Direct globals()['SGLANG_AVAILABLE']: {direct_available}")
        except Exception as e:
            logger.error(f"Error with direct globals access: {e}")
            direct_available = False

        logger.info(f"Final sglang_available decision: {sglang_available}")

        if not sglang_available:
            logger.error("âŒ SGLang is not available - benchmark will use fallback mode")
            logger.error("ðŸ” DIAGNOSIS: This is likely due to CUDA version compatibility issues")
            logger.error("ðŸ“‹ CUDA Status:")
            try:
                import torch
                logger.error(f"   â€¢ PyTorch CUDA available: {torch.cuda.is_available()}")
                if torch.cuda.is_available():
                    logger.error(f"   â€¢ PyTorch CUDA version: {torch.version.cuda}")
                    logger.error(f"   â€¢ GPU Device: {torch.cuda.get_device_name(0)}")
                else:
                    logger.error("   â€¢ PyTorch CUDA: NOT AVAILABLE")
            except:
                logger.error("   â€¢ PyTorch CUDA check failed")

            logger.error("ðŸ”§ POSSIBLE SOLUTIONS:")
            logger.error("   1. Install compatible CUDA toolkit (currently have nvcc 11.5)")
            logger.error("   2. Use PyTorch with CUDA 11.x instead of CUDA 12.x")
            logger.error("   3. Try CPU-only mode: set device='cpu' in SGLang config")
            logger.error("   4. Update SGLang to a version compatible with CUDA 12.x")
            logger.error("âš ï¸ All SGLang operations will use mock inference (no real GPU usage)")
            # Don't raise error, let the benchmark continue with fallback behavior

        logger.info("=== SGLangBenchmark __init__ Debug End ===")

    def run_warmup(self, prompt: str, num_warmup: int = 3):
        logger.info("=== SGLang warmup Debug Start ===")
        logger.info(f"Starting SGLang warmup with prompt: '{prompt[:50]}...', num_warmup: {num_warmup}")

        # Check if SGLang is available (handle variable scope issues)
        logger.info("Checking SGLANG_AVAILABLE for warmup...")
        try:
            sglang_available = globals().get('SGLANG_AVAILABLE', 'GLOBAL_NOT_FOUND')
            logger.info(f"globals().get('SGLANG_AVAILABLE'): {sglang_available}")
        except Exception as e:
            logger.error(f"Error accessing SGLANG_AVAILABLE in warmup: {e}")
            sglang_available = False

        logger.info(f"Using sglang_available: {sglang_available}")

        try:
            if not sglang_available:
                logger.warning("ðŸŸ¡ SGLang not available, using mock warmup (NO GPU USAGE)")
                logger.warning("ðŸ’¡ This means the resource utilization plot will show 0% GPU usage")
                logger.warning("ðŸ’¡ The benchmark is simulating inference without actual GPU computation")
                import time
                time.sleep(0.1 * num_warmup)  # Mock warmup delay
                logger.info("âœ… SGLang mock warmup completed successfully")
                logger.info("=== SGLang warmup Debug End ===")
                return

            if self.engine is None:
                logger.info("Initializing SGLang engine for warmup")
                with self.measure_phase('model_loading'):
                    engine_kwargs = {
                        'model_path': self.model_name,
                        'tp_size': self.tp_size,
                        'max_total_tokens': self.max_total_tokens
                    }
                    if self.max_model_len is not None:
                        engine_kwargs['max_model_len'] = self.max_model_len

                    logger.info(f"Engine kwargs: {engine_kwargs}")

                    try:
                        logger.info(f"ðŸš€ Attempting to create SGLang Engine with kwargs: {engine_kwargs}")
                        
                        # Debug environment before engine creation
                        logger.info("ðŸ” Environment check before engine creation:")
                        logger.info(f"  CUDA_HOME: {os.environ.get('CUDA_HOME', 'NOT_SET')}")
                        logger.info(f"  PATH (first 200 chars): {os.environ.get('PATH', 'NOT_SET')[:200]}")
                        logger.info(f"  LD_LIBRARY_PATH (first 200 chars): {os.environ.get('LD_LIBRARY_PATH', 'NOT_SET')[:200]}")
                        
                        # Debug PyTorch CUDA
                        import torch
                        logger.info(f"  PyTorch CUDA available: {torch.cuda.is_available()}")
                        if torch.cuda.is_available():
                            logger.info(f"  PyTorch CUDA version: {torch.version.cuda}")
                            logger.info(f"  GPU device: {torch.cuda.get_device_name(0)}")
                        
                        # Debug SGLang imports before creating engine
                        logger.info("ðŸ” Testing SGLang components before engine creation:")
                        try:
                            import sglang
                            logger.info(f"  sglang module: {sglang}")
                            
                            import sglang.srt
                            logger.info(f"  sglang.srt module: {sglang.srt}")
                            
                            from sglang.srt.entrypoints.engine import Engine as DebugEngine
                            logger.info(f"  Engine class: {DebugEngine}")
                            logger.info(f"  Engine __init__ signature: {DebugEngine.__init__.__annotations__ if hasattr(DebugEngine.__init__, '__annotations__') else 'No annotations'}")
                            
                        except Exception as import_e:
                            logger.error(f"âŒ Import test failed: {import_e}")
                            logger.error(f"Import traceback: {traceback.format_exc()}")
                        
                        logger.info("ðŸ” About to create Engine instance...")
                        self.engine = Engine(**engine_kwargs)
                        logger.info("âœ… SGLang engine initialized successfully")
                        
                        # Test a basic operation
                        logger.info("ðŸ” Testing basic engine operation...")
                        try:
                            logger.info(f"Engine type: {type(self.engine)}")
                            logger.info(f"Engine attributes: {[attr for attr in dir(self.engine) if not attr.startswith('_')]}")
                        except Exception as test_e:
                            logger.error(f"âŒ Basic engine test failed: {test_e}")
                        
                    except Exception as e:
                        logger.error(f"âŒ Failed to initialize SGLang engine: {e}")
                        logger.error(f"Exception type: {type(e)}")
                        logger.error(f"Exception args: {e.args}")
                        logger.error(f"Full exception traceback: {traceback.format_exc()}")
                        logger.error(f"Engine kwargs used: {engine_kwargs}")
                        
                        # Try to get more specific error info
                        error_msg = str(e)
                        if "CUDA" in error_msg or "cuda" in error_msg:
                            logger.error("ðŸ” CUDA-related error detected:")
                            logger.error("   â€¢ SGLang engine creation failed with CUDA error")
                            logger.error("   â€¢ This could be CUDA version mismatch or missing CUDA libraries")
                        elif "undefined symbol" in error_msg:
                            logger.error("ðŸ” Undefined symbol error detected:")
                            logger.error("   â€¢ This indicates ABI compatibility issues")
                            logger.error("   â€¢ SGLang was likely compiled with different CUDA/C++ versions")
                        else:
                            logger.error("ðŸ” Other engine creation error:")
                            logger.error(f"   â€¢ Error message: {error_msg}")
                            
                        logger.warning("ðŸ”„ SGLang CUDA error detected, switching to fallback mode")
                        logger.warning(f"Setting globals()['SGLANG_AVAILABLE'] = False")
                        globals()['SGLANG_AVAILABLE'] = False
                        self.engine = None
                        logger.info("Fallback mode activated - returning from warmup")
                        # Don't raise error, let it fall back to mock behavior
                        logger.info("=== SGLang warmup Debug End (with engine creation error) ===")
                        return

            logger.info(f"Running {num_warmup} warmup inferences")
            for i in range(num_warmup):
                logger.info(f"Warmup inference {i+1}/{num_warmup}")
                try:
                    result = self.run_inference(prompt)
                    logger.info(f"Warmup inference {i+1} completed successfully")
                except Exception as e:
                    logger.error(f"Warmup inference {i+1} failed: {e}")
                    raise

            logger.info("SGLang warmup completed successfully")

        except Exception as e:
            logger.error(f"SGLang warmup failed: {e}")
            raise

    def run_inference(self, prompt: str) -> tuple[str, float]:
        logger.info("=== SGLang inference Debug Start ===")
        logger.info(f"Starting SGLang inference with prompt: '{prompt[:50]}...'")
        start_time = time.perf_counter()

        # Check if SGLang is available (handle variable scope issues)
        logger.info("Checking SGLANG_AVAILABLE for inference...")
        try:
            sglang_available = globals().get('SGLANG_AVAILABLE', 'GLOBAL_NOT_FOUND')
            logger.info(f"globals().get('SGLANG_AVAILABLE'): {sglang_available}")
        except Exception as e:
            logger.error(f"Error accessing SGLANG_AVAILABLE in inference: {e}")
            sglang_available = False

        logger.info(f"Using sglang_available: {sglang_available}")
        logger.info(f"Current self.engine: {self.engine}")

        try:
            if not sglang_available:
                logger.warning("ðŸŸ¡ SGLang not available, using mock inference (NO GPU USAGE)")
                logger.warning("ðŸ’¡ This means the resource utilization plot will show 0% GPU usage")
                logger.warning("ðŸ’¡ The benchmark is simulating inference without actual GPU computation")
                # Mock inference time based on prompt length
                prompt_tokens = len(prompt.split())
                mock_latency = 0.02 + (prompt_tokens * 0.002)
                logger.info(f"Mock latency calculation: 0.02 + ({prompt_tokens} * 0.002) = {mock_latency}")
                time.sleep(mock_latency)

                end_time = time.perf_counter()
                mock_output = f"SGLang mock response to: {prompt[:30]}..."
                logger.info(f"âœ… Mock inference completed. Output length: {len(mock_output)}, latency: {end_time - start_time:.4f}s")
                logger.info("=== SGLang inference Debug End ===")
                return mock_output, end_time - start_time

            # Ensure engine is initialized
            if self.engine is None:
                logger.info("Engine not initialized, initializing now")
                engine_kwargs = {
                    'model_path': self.model_name,
                    'tp_size': self.tp_size,
                    'max_total_tokens': self.max_total_tokens
                }
                if self.max_model_len is not None:
                    engine_kwargs['max_model_len'] = self.max_model_len

                logger.info(f"Initializing engine with kwargs: {engine_kwargs}")
                try:
                    logger.info(f"ðŸš€ Attempting to create SGLang Engine during inference with kwargs: {engine_kwargs}")
                    self.engine = Engine(**engine_kwargs)
                    logger.info("âœ… Engine initialized successfully")
                except Exception as e:
                    logger.error(f"âŒ Failed to initialize SGLang engine during inference: {e}")
                    logger.error(f"Exception type: {type(e)}")
                    logger.error(f"Exception args: {e.args}")
                    logger.warning("ðŸ”„ SGLang CUDA error detected during inference, switching to fallback mode")
                    logger.warning(f"Setting globals()['SGLANG_AVAILABLE'] = False")
                    globals()['SGLANG_AVAILABLE'] = False
                    self.engine = None
                    # Fall back to mock inference
                    logger.info("Using fallback mock inference...")
                    mock_latency = 0.02 + (prompt_tokens * 0.002)
                    logger.info(f"Mock latency calculation: 0.02 + ({prompt_tokens} * 0.002) = {mock_latency}")
                    time.sleep(mock_latency)
                    end_time = time.perf_counter()
                    mock_output = f"SGLang mock response to: {prompt[:30]}..."
                    logger.info(f"âœ… Fallback mock inference completed. Output length: {len(mock_output)}, latency: {end_time - start_time:.4f}s")
                    logger.info("=== SGLang inference Debug End (with CUDA error) ===")
                    return mock_output, end_time - start_time

            # Check if engine has generate method
            if not hasattr(self.engine, 'generate'):
                logger.error(f"Engine object {type(self.engine)} does not have 'generate' method")
                logger.error(f"Engine attributes: {dir(self.engine) if self.engine else 'None'}")
                raise AttributeError(f"Engine {type(self.engine)} does not have 'generate' method")

            logger.info("Calling engine.generate()")
            response = self.engine.generate(
                prompt=prompt,
                max_tokens=self.max_tokens,
                temperature=0.1,
                top_p=0.95,
                stop=["\n\n", "###"]
            )

            logger.info(f"Engine.generate() returned: {type(response)}")

            end_time = time.perf_counter()
            latency = end_time - start_time

            if isinstance(response, dict) and "text" in response:
                output_text = response["text"]
            else:
                output_text = str(response)

            logger.info(f"Inference completed successfully. Output length: {len(output_text)}, latency: {latency:.4f}s")
            return output_text, latency

        except Exception as e:
            logger.error(f"SGLang inference failed: {e}")
            logger.error(f"Engine type: {type(self.engine)}")
            logger.error(f"Engine is None: {self.engine is None}")
            raise

    def run_concurrent_benchmark(self, prompts: List[str], concurrency: int = 2) -> List[Dict[str, Any]]:
        """Run concurrent benchmark with SGLang."""
        logger.info(f"Running SGLang concurrent benchmark with {len(prompts)} prompts, concurrency: {concurrency}")

        results = []
        for i, prompt in enumerate(prompts):
            logger.info(f"Processing prompt {i+1}/{len(prompts)}")
            if not SGLANG_AVAILABLE:
                # Mock concurrent inference
                prompt_tokens = len(prompt.split())
                mock_latency = 0.02 + (prompt_tokens * 0.002)
                import time
                time.sleep(mock_latency)
                output = f"SGLang mock concurrent response to: {prompt[:30]}..."
                logger.info(f"Mock concurrent inference completed: {len(output)} chars, latency: {mock_latency:.4f}s")
            else:
                output, latency = self.run_inference(prompt)
                mock_latency = latency

            result = {
                "output": output,
                "latency": mock_latency,
                "batch_index": i % concurrency,
                "prompt": prompt
            }
            results.append(result)
            logger.info(f"Prompt {i+1} completed with latency {mock_latency:.4f}s")

        logger.info(f"SGLang concurrent benchmark completed with {len(results)} results")
        return results

    def benchmark_concurrent(self, prompts: List[str], batch_size: int = 4,
                           num_runs: int = 3) -> Dict[str, Any]:
        logger.info(f"Starting benchmark_concurrent with {len(prompts)} prompts, batch_size: {batch_size}, num_runs: {num_runs}")

        results = {
            'model': self.model_name,
            'batch_size': batch_size,
            'concurrent_metrics': [],
            'system_info': self.monitor.get_system_info() if hasattr(self, 'monitor') else {}
        }

        if self.engine is None:
            logger.info("Initializing engine in benchmark_concurrent")
            with self.measure_phase('model_loading'):
                engine_kwargs = {
                    'model_path': self.model_name,
                    'tp_size': self.tp_size,
                    'max_total_tokens': self.max_total_tokens
                }
                if self.max_model_len is not None:
                    engine_kwargs['max_model_len'] = self.max_model_len

                logger.info(f"Engine kwargs for concurrent benchmark: {engine_kwargs}")

                try:
                    self.engine = Engine(**engine_kwargs)
                    logger.info("Engine initialized successfully for concurrent benchmark")
                except Exception as e:
                    logger.error(f"Failed to initialize engine for concurrent benchmark: {e}")
                    raise

        for batch_start in range(0, len(prompts), batch_size):
            batch_prompts = prompts[batch_start:batch_start + batch_size]
            batch_latencies = []

            for _ in range(num_runs):
                start_time = time.perf_counter()

                responses = []
                for prompt in batch_prompts:
                    response = self.engine.generate(
                        prompt=prompt,
                        max_tokens=self.max_tokens,
                        temperature=0.1,
                        top_p=0.95,
                        stop=["\n\n", "###"]
                    )
                    responses.append(response)

                end_time = time.perf_counter()
                batch_latency = end_time - start_time
                batch_latencies.append(batch_latency)

                total_input_tokens = sum(len(p.split()) for p in batch_prompts)
                total_output_tokens = sum(
                    len(r["text"].split()) if isinstance(r, dict) and "text" in r else len(str(r).split())
                    for r in responses
                )
                total_tokens = total_input_tokens + total_output_tokens

            if batch_latencies:
                from src.core.benchmark_core import InferenceMetrics
                metrics = InferenceMetrics(
                    ttft=min(batch_latencies),
                    tpot=sum(batch_latencies) / len(batch_latencies),
                    throughput=total_tokens / sum(batch_latencies),
                    total_tokens=total_tokens,
                    input_tokens=total_input_tokens,
                    output_tokens=total_output_tokens,
                    p50_latency=sorted(batch_latencies)[len(batch_latencies)//2],
                    p95_latency=sorted(batch_latencies)[int(len(batch_latencies)*0.95)] if batch_latencies else max(batch_latencies),
                    p99_latency=sorted(batch_latencies)[int(len(batch_latencies)*0.99)] if batch_latencies else max(batch_latencies),
                    gpu_memory_used=self.monitor.get_gpu_memory() if hasattr(self, 'monitor') else 0,
                    gpu_utilization=self.monitor.get_gpu_utilization() if hasattr(self, 'monitor') else 0,
                    cpu_utilization=self.monitor.get_cpu_utilization() if hasattr(self, 'monitor') else 0
                )
                results['concurrent_metrics'].append(metrics)

        return results

    def benchmark_streaming(self, prompt: str, num_runs: int = 3) -> Dict[str, Any]:
        results = {
            'model': self.model_name,
            'streaming_analysis': [],
            'system_info': self.monitor.get_system_info() if hasattr(self, 'monitor') else {}
        }

        if self.engine is None:
            with self.measure_phase('model_loading'):
                engine_kwargs = {
                    'model_path': self.model_name,
                    'tp_size': self.tp_size,
                    'max_total_tokens': self.max_total_tokens
                }
                if self.max_model_len is not None:
                    engine_kwargs['max_model_len'] = self.max_model_len

                self.engine = Engine(**engine_kwargs)

        for _ in range(num_runs):
            start_time = time.perf_counter()
            token_latencies = []
            tokens_received = 0

            try:
                for partial_response in self.engine.generate_stream(
                    prompt=prompt,
                    max_tokens=self.max_tokens,
                    temperature=0.1,
                    top_p=0.95
                ):
                    if partial_response:
                        token_time = time.perf_counter()
                        token_latencies.append(token_time - start_time)
                        tokens_received += 1
                        start_time = token_time

            except Exception as e:
                print(f"Streaming error: {e}")
                continue

            if token_latencies:
                from src.core.benchmark_core import InferenceMetrics
                metrics = InferenceMetrics(
                    ttft=token_latencies[0] if token_latencies else 0,
                    tpot=sum(token_latencies) / len(token_latencies) if token_latencies else 0,
                    throughput=tokens_received / sum(token_latencies) if token_latencies else 0,
                    total_tokens=len(prompt.split()) + tokens_received,
                    input_tokens=len(prompt.split()),
                    output_tokens=tokens_received,
                    p50_latency=sorted(token_latencies)[len(token_latencies)//2],
                    p95_latency=sorted(token_latencies)[int(len(token_latencies)*0.95)] if len(token_latencies) > 19 else max(token_latencies) if token_latencies else 0,
                    p99_latency=sorted(token_latencies)[int(len(token_latencies)*0.99)] if len(token_latencies) > 99 else max(token_latencies) if token_latencies else 0,
                    gpu_memory_used=self.monitor.get_gpu_memory() if hasattr(self, 'monitor') else 0,
                    gpu_utilization=self.monitor.get_gpu_utilization() if hasattr(self, 'monitor') else 0,
                    cpu_utilization=self.monitor.get_cpu_utilization() if hasattr(self, 'monitor') else 0
                )

                results['streaming_analysis'].append({
                    'run': len(results['streaming_analysis']),
                    'metrics': metrics,
                    'tokens_per_second': tokens_received / sum(token_latencies) if token_latencies else 0
                })

        return results

    def get_engine_info(self) -> Dict[str, Any]:
        """Get information about the SGLang engine configuration."""
        info = {
            "engine": "SGLang",
            "version": "0.5.1.post3",
            "device": "cuda",
            "tp_size": self.tp_size,
            "max_model_len": self.max_model_len,
            "max_total_tokens": self.max_total_tokens,
            "features": [
                "Radix Attention",
                "Compression",
                "Efficient Serving",
                "Advanced Scheduling",
                "Token Attention"
            ]
        }

        if not SGLANG_AVAILABLE:
            info["status"] = "FALLBACK_MODE"
            info["note"] = "CUDA compatibility issue - using mock implementation"
            info["features"] = ["Mock Implementation"]

        return info
