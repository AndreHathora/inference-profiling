# LLM Inference Latency Breakdown

## 1. Input Processing & Tokenization

## 2. Model Loading & Initialization

## 3. Prefill Phase

## 4. Decode Phase

## 5. Output Processing

## 6. Memory Operations
